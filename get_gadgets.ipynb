{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff021db7-7347-4ab9-a5cd-a73edf7b3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import isodate\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pandas import Timestamp\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"YOUTUBE_KEY\")\n",
    "BASE_URL = \"https://www.googleapis.com/youtube/v3\"\n",
    "\n",
    "# Display the DataFrame\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "def get_channel_id_from_username(username, api_key):\n",
    "    channel_username = username.lstrip('@')\n",
    "    api_url = f\"https://www.googleapis.com/youtube/v3/search?part=snippet&type=channel&q={channel_username}&key={api_key}\"\n",
    "    response = requests.get(api_url)\n",
    "    data = response.json()\n",
    "    if 'items' in data and data['items']:\n",
    "        return data['items'][0]['snippet']['channelId']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_channel_upload_playlist_id(channel_id):\n",
    "    url = f\"{BASE_URL}/channels?part=contentDetails&id={channel_id}&key={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    if 'error' in data:\n",
    "        raise ValueError(f\"API Error: {data['error']['message']}\")\n",
    "    items = data.get('items', [])\n",
    "    if not items:\n",
    "        raise ValueError(f\"No channel found for ID: {channel_id}\")\n",
    "    return items[0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "\n",
    "def get_latest_channel_videos(channel_id, max_videos=100):\n",
    "    video_ids = []\n",
    "    playlist_id = get_channel_upload_playlist_id(channel_id)\n",
    "    next_page_token = None\n",
    "    \n",
    "    while len(video_ids) < max_videos:\n",
    "        url = f\"{BASE_URL}/playlistItems?part=snippet&playlistId={playlist_id}&maxResults=50&key={API_KEY}\"\n",
    "        if next_page_token:\n",
    "            url += f\"&pageToken={next_page_token}\"\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'error' in data:\n",
    "            raise ValueError(f\"API Error: {data['error']['message']}\")\n",
    "        \n",
    "        items = data.get('items', [])\n",
    "        if not items:\n",
    "            break\n",
    "        \n",
    "        for item in items:\n",
    "            video_ids.append(item['snippet']['resourceId']['videoId'])\n",
    "            if len(video_ids) >= max_videos:\n",
    "                break\n",
    "        \n",
    "        if len(video_ids) >= max_videos:\n",
    "            break\n",
    "        \n",
    "        next_page_token = data.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        \n",
    "        print(f\"Videos found so far: {len(video_ids)}\")\n",
    "        time.sleep(1)  # To avoid hitting API rate limits\n",
    "    \n",
    "    print(f\"Total video IDs found: {len(video_ids)}\")\n",
    "    return video_ids[:max_videos]\n",
    "\n",
    "def get_video_details(video_ids):\n",
    "    videos = []\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        batch = video_ids[i:i+50]\n",
    "        url = f\"{BASE_URL}/videos?part=snippet,contentDetails,statistics&id={','.join(batch)}&key={API_KEY}\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'error' in data:\n",
    "            raise ValueError(f\"API Error: {data['error']['message']}\")\n",
    "        \n",
    "        items = data.get('items', [])\n",
    "        if not items:\n",
    "            print(f\"No details found for video IDs: {batch}\")\n",
    "            continue\n",
    "        \n",
    "        for item in items:\n",
    "            try:\n",
    "                duration = isodate.parse_duration(item['contentDetails']['duration'])\n",
    "                published_at = item['snippet'].get('publishedAt')\n",
    "                if published_at:\n",
    "                    published_at = pd.to_datetime(published_at).tz_convert('US/Pacific')\n",
    "                video = {\n",
    "                    'videoId': item['id'],\n",
    "                    'title': item['snippet']['title'],\n",
    "                    'publishedAt': published_at,\n",
    "                    'viewCount': int(item['statistics'].get('viewCount', 0)),\n",
    "                    'likeCount': int(item['statistics'].get('likeCount', 0)),\n",
    "                    'commentCount': int(item['statistics'].get('commentCount', 0)),\n",
    "                    'duration': str(duration),\n",
    "                    'description': item['snippet']['description'],\n",
    "                    'tags': ', '.join(item['snippet'].get('tags', [])),\n",
    "                    'thumbnailUrl': item['snippet']['thumbnails']['default']['url']\n",
    "                }\n",
    "                videos.append(video)\n",
    "            except KeyError as e:\n",
    "                print(f\"KeyError for video {item.get('id', 'Unknown')}: {str(e)}\")\n",
    "        \n",
    "        time.sleep(1)  # To avoid hitting API rate limits\n",
    "    \n",
    "    return videos\n",
    "\n",
    "def fetch_latest_channel_videos(channel_id, max_videos=100):\n",
    "    try:\n",
    "        video_ids = get_latest_channel_videos(channel_id, max_videos)\n",
    "        if not video_ids:\n",
    "            print(\"No video IDs found.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        videos = get_video_details(video_ids)\n",
    "        if not videos:\n",
    "            print(\"No video details found.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = pd.DataFrame(videos)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def process_channels(channel_usernames, max_videos_per_channel=100):\n",
    "    all_videos = []\n",
    "    for username in channel_usernames:\n",
    "        print(f\"Processing channel: {username}\")\n",
    "        channel_id = get_channel_id_from_username(username, API_KEY)\n",
    "        if channel_id:\n",
    "            df = fetch_latest_channel_videos(channel_id, max_videos_per_channel)\n",
    "            if not df.empty:\n",
    "                df['channel'] = username\n",
    "                all_videos.append(df)\n",
    "        else:\n",
    "            print(f\"Could not find channel ID for username: {username}\")\n",
    "    \n",
    "    if all_videos:\n",
    "        combined_df = pd.concat(all_videos, ignore_index=True)\n",
    "        combined_df = combined_df.sort_values('publishedAt', ascending=False)\n",
    "        \n",
    "        print(f\"Total videos fetched: {len(combined_df)}\")\n",
    "        combined_df.to_csv(\"yt_channels_latest_videos.csv\", index=False)\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No videos found for any channels.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def parse_date(date_value):\n",
    "    if pd.isna(date_value):\n",
    "        return pd.NaT\n",
    "    \n",
    "    if isinstance(date_value, (int, float)):\n",
    "        return pd.NaT\n",
    "    \n",
    "    try:\n",
    "        # Try parsing as ISO format first\n",
    "        date = datetime.fromisoformat(date_value.replace('Z', '+00:00'))\n",
    "        return date.astimezone(pytz.UTC)\n",
    "    except ValueError:\n",
    "        try:\n",
    "            # Try parsing with explicit format\n",
    "            parts = date_value.split()\n",
    "            if len(parts) == 3:\n",
    "                date_part, time_part, tz_part = parts\n",
    "                dt = datetime.strptime(f\"{date_part} {time_part}\", \"%Y-%m-%d %H:%M:%S\")\n",
    "                \n",
    "                if tz_part in ['PDT', 'PST']:\n",
    "                    tz = pytz.timezone('US/Pacific')\n",
    "                else:\n",
    "                    tz = pytz.timezone('UTC')\n",
    "                \n",
    "                return tz.localize(dt).astimezone(pytz.UTC)\n",
    "            elif len(parts) == 2:\n",
    "                # Handle case where there's no timezone\n",
    "                date_part, time_part = parts\n",
    "                dt = datetime.strptime(f\"{date_part} {time_part}\", \"%Y-%m-%d %H:%M:%S\")\n",
    "                return pytz.UTC.localize(dt)\n",
    "            else:\n",
    "                # If we can't parse it, return NaT\n",
    "                return pd.NaT\n",
    "        except Exception:\n",
    "            # If all parsing attempts fail, return NaT\n",
    "            return pd.NaT\n",
    "\n",
    "    \n",
    "def safe_duration_to_seconds(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    if isinstance(x, (int, float)):\n",
    "        return x  # Assume it's already in seconds\n",
    "    try:\n",
    "        parts = x.split(':')\n",
    "        return sum(int(part) * 60**i for i, part in enumerate(reversed(parts)))\n",
    "    except:\n",
    "        return np.nan \n",
    "\n",
    "# Function to fit for the curved line\n",
    "def curve_func(x, a, b, c):\n",
    "    return a * (x ** b) + c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff7c476f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel_usernames = ['unboxtherapy', 'mkbhd', 'mrwhosetheboss', 'linustechtips', 'austinevans', 'uravgconsumer', 'mrmobile', 'flossycarter', 'techtablets', 'phonebuff']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import re\n",
    "\n",
    "def get_top_tech_review_channels():\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "\n",
    "    # Access the API key\n",
    "    api_key = os.environ.get(\"PERPLEXITY_API_KEY\")\n",
    "\n",
    "    if not api_key:\n",
    "        raise ValueError(\"PERPLEXITY_API_KEY not found in environment variables\")\n",
    "\n",
    "    url = \"https://api.perplexity.ai/chat/completions\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"llama-3.1-sonar-huge-128k-online\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Be precise and concise. Provide the answer as a Python list of YouTube channel usernames as they appear at the end of the URL.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What are the best YouTube channels that provide the best product reviews for tech gadgets in terms of popularity and likes? Format the output as a Python list of channel usernames as they appear at the end of the URL (e.g., ['mkbhd', 'unboxtherapy']).\"\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9,\n",
    "        \"return_citations\": True,\n",
    "        \"return_images\": False,\n",
    "        \"return_related_questions\": False,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        response.raise_for_status()  # Raises a HTTPError if the status is 4xx, 5xx\n",
    "\n",
    "        result = response.json()\n",
    "        content = result['choices'][0]['message']['content']\n",
    "        \n",
    "        # Extract the list from the content using regex\n",
    "        match = re.search(r'\\[.*?\\]', content, re.DOTALL)\n",
    "        if match:\n",
    "            channel_list = match.group(0)\n",
    "            return eval(channel_list)  # Safely evaluate the string as a Python list\n",
    "        else:\n",
    "            return []  # Return an empty list if no match is found\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"An error occurred while making the API request: {e}\")\n",
    "        return []\n",
    "    except (KeyError, IndexError) as e:\n",
    "        print(f\"An error occurred while parsing the API response: {e}\")\n",
    "        return []\n",
    "    except SyntaxError as e:\n",
    "        print(f\"An error occurred while evaluating the channel list: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "channel_usernames = get_top_tech_review_channels()\n",
    "print(\"channel_usernames =\", channel_usernames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5eb4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of channel usernames\n",
    "channel_usernames = [\"mkbhd\", \"unboxtherapy\", \"LinusTechTips\", \"Mrwhosetheboss\", \"UrAvgConsumer\", \"austinevans\", \"TechnicalGuruji\", \"TLD\", \"technobuffalo\"]  # Add more channels as needed\n",
    "\n",
    "# Process all channels\n",
    "result_df = process_channels(channel_usernames, max_videos_per_channel=100)\n",
    "\n",
    "# Display the first row of the result\n",
    "result_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa0c90d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoId</th>\n",
       "      <th>title</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>duration</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>thumbnailUrl</th>\n",
       "      <th>channel</th>\n",
       "      <th>hours_since_published</th>\n",
       "      <th>likes_per_view</th>\n",
       "      <th>comments_per_view</th>\n",
       "      <th>views_by_hr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uiTDtKFh3pE</td>\n",
       "      <td>The WIRELESS Gaming PC</td>\n",
       "      <td>2024-09-14 17:03:03+00:00</td>\n",
       "      <td>16877.0</td>\n",
       "      <td>1172.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0:00:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.ytimg.com/vi/uiTDtKFh3pE/default.jpg</td>\n",
       "      <td>austinevans</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       videoId                   title               publishedAt  viewCount  \\\n",
       "0  uiTDtKFh3pE  The WIRELESS Gaming PC 2024-09-14 17:03:03+00:00    16877.0   \n",
       "\n",
       "   likeCount  commentCount duration description tags  \\\n",
       "0     1172.0          38.0  0:00:47         NaN  NaN   \n",
       "\n",
       "                                     thumbnailUrl      channel  \\\n",
       "0  https://i.ytimg.com/vi/uiTDtKFh3pE/default.jpg  austinevans   \n",
       "\n",
       "   hours_since_published  likes_per_view  comments_per_view  views_by_hr  \n",
       "0                    0.0        0.069444           0.002252          inf  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = result_df\n",
    "df = pd.read_csv(\"yt_channels_latest_videos.csv\")\n",
    "\n",
    "# Apply the modified function\n",
    "df['publishedAt'] = df['publishedAt'].apply(parse_date)\n",
    "\n",
    "# Find the most recent date in the dataset\n",
    "most_recent_date = df['publishedAt'].max()\n",
    "\n",
    "# Calculate hours since published relative to the most recent date\n",
    "df['hours_since_published'] = (most_recent_date - df['publishedAt']).dt.total_seconds() / 3600\n",
    "\n",
    "# Calculate likes per view\n",
    "df['likes_per_view'] = df['likeCount'] / df['viewCount']\n",
    "\n",
    "df['commentCount'] = pd.to_numeric(df['commentCount'], errors='coerce')\n",
    "df['viewCount'] = pd.to_numeric(df['viewCount'], errors='coerce')\n",
    "\n",
    "# Now calculate comments per view\n",
    "df['comments_per_view'] = df['commentCount'] / df['viewCount']\n",
    "\n",
    "# If you also need to calculate views_by_hr, make sure 'hours_since_published' is numeric too\n",
    "df['hours_since_published'] = pd.to_numeric(df['hours_since_published'], errors='coerce')\n",
    "df['views_by_hr'] = df['viewCount'] / df['hours_since_published']\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a42e990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qs/72735_yn25sfwrwf0ryfwcb40000gn/T/ipykernel_20067/2718769407.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['duration_seconds'] = df_filtered['duration'].apply(safe_duration_to_seconds)\n",
      "/var/folders/qs/72735_yn25sfwrwf0ryfwcb40000gn/T/ipykernel_20067/2718769407.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['log_comments'] = np.log1p(df_filtered['commentCount'])\n",
      "/var/folders/qs/72735_yn25sfwrwf0ryfwcb40000gn/T/ipykernel_20067/2718769407.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['combined_metric'] = df_filtered['views_by_hr'] * df_filtered['likes_per_view']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "one_month_ago = datetime.now(pytz.UTC) - timedelta(days=30)\n",
    "df_filtered = df[\n",
    "    (pd.to_datetime(df['publishedAt'], utc=True) < one_month_ago) &\n",
    "    (df['duration'].apply(safe_duration_to_seconds) > 120)\n",
    "]\n",
    "\n",
    "df_filtered['duration_seconds'] = df_filtered['duration'].apply(safe_duration_to_seconds)\n",
    "\n",
    "# Normalize duration for marker size (smallest stays the same, largest is 9 times as large)\n",
    "min_duration = df_filtered['duration_seconds'].min()\n",
    "max_duration = df_filtered['duration_seconds'].max()\n",
    "normalized_duration = (df_filtered['duration_seconds'] - min_duration) / (max_duration - min_duration)\n",
    "marker_sizes = 5 + normalized_duration * 40  # Scale from 5 to 45\n",
    "\n",
    "# Log normalize the number of comments\n",
    "df_filtered['log_comments'] = np.log1p(df_filtered['commentCount'])\n",
    "\n",
    "# Calculate combined metric for top 5%\n",
    "df_filtered['combined_metric'] = df_filtered['views_by_hr'] * df_filtered['likes_per_view']\n",
    "threshold_value = df_filtered['combined_metric'].quantile(0.95)\n",
    "\n",
    "# Fit the curve\n",
    "x_data = df_filtered['views_by_hr']\n",
    "y_data = df_filtered['likes_per_view']\n",
    "popt, _ = optimize.curve_fit(curve_func, x_data[df_filtered['combined_metric'] > threshold_value], \n",
    "                             y_data[df_filtered['combined_metric'] > threshold_value], \n",
    "                             p0=[1, -0.5, 0.001], maxfev=10000)\n",
    "\n",
    "# Add some jitter to reduce overlapping\n",
    "jitter_x = np.random.normal(0, 0.01, len(df_filtered))\n",
    "jitter_y = np.random.normal(0, 0.01, len(df_filtered))\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add scatter plot\n",
    "scatter = go.Scatter(\n",
    "    x=df_filtered['views_by_hr'] * (1 + jitter_x),\n",
    "    y=df_filtered['likes_per_view'] * (1 + jitter_y),\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=marker_sizes,\n",
    "        color=df_filtered['log_comments'],\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(title='Log(Number of Comments + 1)'),\n",
    "        showscale=True,\n",
    "        opacity=0.7\n",
    "    ),\n",
    "    text=df_filtered['title'],\n",
    "    hovertemplate=\n",
    "    \"<b>%{text}</b><br>\" +\n",
    "    \"Views per Hour: %{x:.2f}<br>\" +\n",
    "    \"Likes per View: %{y:.4f}<br>\" +\n",
    "    \"Duration: %{marker.size:.0f} seconds<br>\" +\n",
    "    \"Number of Comments: %{marker.color:.0f}<br>\" +\n",
    "    \"<extra></extra>\",\n",
    "    name='Videos'\n",
    ")\n",
    "\n",
    "fig.add_trace(scatter)\n",
    "\n",
    "# Add curved line to separate top 5%\n",
    "x_range = np.logspace(np.log10(df_filtered['views_by_hr'].min()), np.log10(df_filtered['views_by_hr'].max()), 1000)\n",
    "y_curve = curve_func(x_range, *popt)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_range,\n",
    "    y=y_curve,\n",
    "    mode='lines',\n",
    "    line=dict(color='red', dash='dash'),\n",
    "    name='Top 5% Threshold'\n",
    "))\n",
    "\n",
    "# Calculate the maximum y-value\n",
    "max_y = df_filtered['likes_per_view'].max() * 1.1\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    title='Views per Hour vs Likes per View (Videos > 1 Month Old, > 2 Minutes)',\n",
    "    xaxis_title='Views per Hour',\n",
    "    yaxis_title='Likes per View',\n",
    "    height=800,\n",
    "    width=1200,\n",
    "    hovermode='closest',\n",
    "    template='plotly_white',\n",
    "    yaxis=dict(range=[np.log10(df_filtered['likes_per_view'].min()), np.log10(max_y)]),\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    ),\n",
    "    margin=dict(r=120),  # Increase right margin to accommodate colorbar\n",
    ")\n",
    "\n",
    "\n",
    "# Update axes to logarithmic scale\n",
    "fig.update_xaxes(type=\"log\")\n",
    "fig.update_yaxes(type=\"log\")\n",
    "\n",
    "# Add a colorbar title\n",
    "fig.update_coloraxes(\n",
    "    colorbar_title_text='Log(Number of Comments + 1)',\n",
    "    colorbar=dict(\n",
    "        len=0.75,  # Reduce length of colorbar\n",
    "        yanchor=\"top\",\n",
    "        y=1,\n",
    "        x=1.05,  # Move colorbar to the right\n",
    "        thickness=20,  # Adjust thickness of colorbar\n",
    "        title=dict(side=\"right\", font=dict(size=12))  # Move title to the right side\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add a note about marker size\n",
    "fig.add_annotation(\n",
    "    text=\"Marker size represents video duration\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=1.02, y=1.05,\n",
    "    showarrow=False,\n",
    "    font=dict(size=10),\n",
    "    align=\"left\"\n",
    ")\n",
    "\n",
    "pio.write_html(fig, file='plot.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de60f873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qs/72735_yn25sfwrwf0ryfwcb40000gn/T/ipykernel_20067/75614639.py:12: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoId</th>\n",
       "      <th>title</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>duration</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>thumbnailUrl</th>\n",
       "      <th>channel</th>\n",
       "      <th>hours_since_published</th>\n",
       "      <th>likes_per_view</th>\n",
       "      <th>comments_per_view</th>\n",
       "      <th>views_by_hr</th>\n",
       "      <th>duration_seconds</th>\n",
       "      <th>log_comments</th>\n",
       "      <th>combined_metric</th>\n",
       "      <th>engagement_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>kqcphMpZ4sU</td>\n",
       "      <td>World's Most High-Tech Wedding Speech!</td>\n",
       "      <td>2024-06-21 14:43:30+00:00</td>\n",
       "      <td>3188185.0</td>\n",
       "      <td>215610.0</td>\n",
       "      <td>18002.0</td>\n",
       "      <td>0:15:43</td>\n",
       "      <td>This will always be one of the high points of my life. I’m so happy ❤️\\nThanks to @TheWeddingFil...</td>\n",
       "      <td>Wedding, Speech, Marriage, Girlfriend, Wife, Fiance</td>\n",
       "      <td>https://i.ytimg.com/vi/kqcphMpZ4sU/default.jpg</td>\n",
       "      <td>Mrwhosetheboss</td>\n",
       "      <td>2042.325833</td>\n",
       "      <td>0.067628</td>\n",
       "      <td>0.005646</td>\n",
       "      <td>1561.056002</td>\n",
       "      <td>943</td>\n",
       "      <td>9.798294</td>\n",
       "      <td>105.570814</td>\n",
       "      <td>0.073274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>a7ItChBrY8E</td>\n",
       "      <td>I Gave A Commencement Speech!</td>\n",
       "      <td>2024-05-23 19:41:23+00:00</td>\n",
       "      <td>1286436.0</td>\n",
       "      <td>84459.0</td>\n",
       "      <td>3883.0</td>\n",
       "      <td>0:08:58</td>\n",
       "      <td>Commencement for the class of 2024 at the school I graduated from!\\n\\nMKBHD Merch: http://shop.M...</td>\n",
       "      <td>MKBHD, commencement, commencement speech, 2024 commencement, Stevens, Stevens Tech, Stevens Inst...</td>\n",
       "      <td>https://i.ytimg.com/vi/a7ItChBrY8E/default.jpg</td>\n",
       "      <td>mkbhd</td>\n",
       "      <td>2733.361111</td>\n",
       "      <td>0.065653</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>470.642534</td>\n",
       "      <td>538</td>\n",
       "      <td>8.264621</td>\n",
       "      <td>30.899320</td>\n",
       "      <td>0.068672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>P6rH5b479qw</td>\n",
       "      <td>Tech Experts React to Bad &amp; Great Tech in Movies - Episode 1</td>\n",
       "      <td>2024-06-17 17:08:26+00:00</td>\n",
       "      <td>1480413.0</td>\n",
       "      <td>92794.0</td>\n",
       "      <td>5086.0</td>\n",
       "      <td>0:21:43</td>\n",
       "      <td>Visit https://www.squarespace.com/LTT and use offer code LTT for 10% off\\n\\nCheck out the UGREEN...</td>\n",
       "      <td>Tech, Support, React, Movies, TV Show, Bones, NCIS, Level 1 Techs, Wendell, Gaming, Computer, Se...</td>\n",
       "      <td>https://i.ytimg.com/vi/P6rH5b479qw/default.jpg</td>\n",
       "      <td>LinusTechTips</td>\n",
       "      <td>2135.910278</td>\n",
       "      <td>0.062681</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>693.106361</td>\n",
       "      <td>1303</td>\n",
       "      <td>8.534444</td>\n",
       "      <td>43.444709</td>\n",
       "      <td>0.066117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>Y5Ud5HKO2C4</td>\n",
       "      <td>Jonathan Morrison Lost Everything. We Respond.</td>\n",
       "      <td>2024-06-27 15:01:40+00:00</td>\n",
       "      <td>897034.0</td>\n",
       "      <td>53582.0</td>\n",
       "      <td>1837.0</td>\n",
       "      <td>0:18:16</td>\n",
       "      <td>WATCH @TLD's video: https://youtu.be/qS7vnaLICrw\\n\\nHUGE thank you to Amaran, Sony and everyone ...</td>\n",
       "      <td>jonathan morrison, jon, jonathan, morrison, tld, tld today, tldtoday, austin, austin evans</td>\n",
       "      <td>https://i.ytimg.com/vi/Y5Ud5HKO2C4/default.jpg</td>\n",
       "      <td>austinevans</td>\n",
       "      <td>1898.023056</td>\n",
       "      <td>0.059732</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>472.614912</td>\n",
       "      <td>1096</td>\n",
       "      <td>7.516433</td>\n",
       "      <td>28.230426</td>\n",
       "      <td>0.061780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>GdQ5bClEgHg</td>\n",
       "      <td>Hello, old friend… - Media Ripping Explained</td>\n",
       "      <td>2024-08-12 18:35:14+00:00</td>\n",
       "      <td>1700758.0</td>\n",
       "      <td>97458.0</td>\n",
       "      <td>5540.0</td>\n",
       "      <td>0:13:34</td>\n",
       "      <td>Get a 15-day free trial for unlimited backup at https://www.backblaze.com/landing/podcast-ltt.ht...</td>\n",
       "      <td>piracy, blu-ray, 4k video, how to pirate, how to rip discs, netflix, prime, amazon, disney, disn...</td>\n",
       "      <td>https://i.ytimg.com/vi/GdQ5bClEgHg/default.jpg</td>\n",
       "      <td>LinusTechTips</td>\n",
       "      <td>790.463611</td>\n",
       "      <td>0.057303</td>\n",
       "      <td>0.003257</td>\n",
       "      <td>2151.595565</td>\n",
       "      <td>814</td>\n",
       "      <td>8.619930</td>\n",
       "      <td>123.292203</td>\n",
       "      <td>0.060560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         videoId  \\\n",
       "366  kqcphMpZ4sU   \n",
       "465  a7ItChBrY8E   \n",
       "380  P6rH5b479qw   \n",
       "342  Y5Ud5HKO2C4   \n",
       "150  GdQ5bClEgHg   \n",
       "\n",
       "                                                            title  \\\n",
       "366                        World's Most High-Tech Wedding Speech!   \n",
       "465                                 I Gave A Commencement Speech!   \n",
       "380  Tech Experts React to Bad & Great Tech in Movies - Episode 1   \n",
       "342                Jonathan Morrison Lost Everything. We Respond.   \n",
       "150                  Hello, old friend… - Media Ripping Explained   \n",
       "\n",
       "                  publishedAt  viewCount  likeCount  commentCount duration  \\\n",
       "366 2024-06-21 14:43:30+00:00  3188185.0   215610.0       18002.0  0:15:43   \n",
       "465 2024-05-23 19:41:23+00:00  1286436.0    84459.0        3883.0  0:08:58   \n",
       "380 2024-06-17 17:08:26+00:00  1480413.0    92794.0        5086.0  0:21:43   \n",
       "342 2024-06-27 15:01:40+00:00   897034.0    53582.0        1837.0  0:18:16   \n",
       "150 2024-08-12 18:35:14+00:00  1700758.0    97458.0        5540.0  0:13:34   \n",
       "\n",
       "                                                                                             description  \\\n",
       "366  This will always be one of the high points of my life. I’m so happy ❤️\\nThanks to @TheWeddingFil...   \n",
       "465  Commencement for the class of 2024 at the school I graduated from!\\n\\nMKBHD Merch: http://shop.M...   \n",
       "380  Visit https://www.squarespace.com/LTT and use offer code LTT for 10% off\\n\\nCheck out the UGREEN...   \n",
       "342  WATCH @TLD's video: https://youtu.be/qS7vnaLICrw\\n\\nHUGE thank you to Amaran, Sony and everyone ...   \n",
       "150  Get a 15-day free trial for unlimited backup at https://www.backblaze.com/landing/podcast-ltt.ht...   \n",
       "\n",
       "                                                                                                    tags  \\\n",
       "366                                                  Wedding, Speech, Marriage, Girlfriend, Wife, Fiance   \n",
       "465  MKBHD, commencement, commencement speech, 2024 commencement, Stevens, Stevens Tech, Stevens Inst...   \n",
       "380  Tech, Support, React, Movies, TV Show, Bones, NCIS, Level 1 Techs, Wendell, Gaming, Computer, Se...   \n",
       "342           jonathan morrison, jon, jonathan, morrison, tld, tld today, tldtoday, austin, austin evans   \n",
       "150  piracy, blu-ray, 4k video, how to pirate, how to rip discs, netflix, prime, amazon, disney, disn...   \n",
       "\n",
       "                                       thumbnailUrl         channel  \\\n",
       "366  https://i.ytimg.com/vi/kqcphMpZ4sU/default.jpg  Mrwhosetheboss   \n",
       "465  https://i.ytimg.com/vi/a7ItChBrY8E/default.jpg           mkbhd   \n",
       "380  https://i.ytimg.com/vi/P6rH5b479qw/default.jpg   LinusTechTips   \n",
       "342  https://i.ytimg.com/vi/Y5Ud5HKO2C4/default.jpg     austinevans   \n",
       "150  https://i.ytimg.com/vi/GdQ5bClEgHg/default.jpg   LinusTechTips   \n",
       "\n",
       "     hours_since_published  likes_per_view  comments_per_view  views_by_hr  \\\n",
       "366            2042.325833        0.067628           0.005646  1561.056002   \n",
       "465            2733.361111        0.065653           0.003018   470.642534   \n",
       "380            2135.910278        0.062681           0.003436   693.106361   \n",
       "342            1898.023056        0.059732           0.002048   472.614912   \n",
       "150             790.463611        0.057303           0.003257  2151.595565   \n",
       "\n",
       "     duration_seconds  log_comments  combined_metric  engagement_metric  \n",
       "366               943      9.798294       105.570814           0.073274  \n",
       "465               538      8.264621        30.899320           0.068672  \n",
       "380              1303      8.534444        43.444709           0.066117  \n",
       "342              1096      7.516433        28.230426           0.061780  \n",
       "150               814      8.619930       123.292203           0.060560  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_filtered is your original DataFrame from the previous code\n",
    "\n",
    "# Step 1: Sort the DataFrame by 'combined_metric' in descending order\n",
    "df_sorted = df_filtered.sort_values('combined_metric', ascending=False)\n",
    "\n",
    "# Step 2: Select the top 25%\n",
    "top_25_percent = df_sorted.head(int(len(df_sorted) * 0.25))\n",
    "\n",
    "# Step 3: Add a new column that combines 'likes_per_view' and 'comments_per_view'\n",
    "top_25_percent['engagement_metric'] = top_25_percent['likes_per_view'] + top_25_percent['comments_per_view']\n",
    "\n",
    "# Step 4: Sort the resulting DataFrame by the new 'engagement_metric' column in descending order\n",
    "top_25_percent = top_25_percent.sort_values('engagement_metric', ascending=False)\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame\n",
    "top_25_percent.head()\n",
    "\n",
    "# If you want to save this DataFrame to a CSV file, you can uncomment the following line:\n",
    "# result_df.to_csv('top_25_percent_engagement.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51640b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 videos that are likely tech product reviews or unboxings:\n",
      "['zijUjJdegI8']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up OpenAI client for Solar API\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"SOLAR_API_KEY\"),\n",
    "    base_url=\"https://api.upstage.ai/v1/solar\"\n",
    ")\n",
    "\n",
    "def is_tech_review(title):\n",
    "\n",
    "    \n",
    "    # Combine the title, description, and tags into a single text\n",
    "    content = f\"Title: {title}\"\n",
    "    \n",
    "    # Use Solar to analyze the content\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following video content and determine if it's likely to be a recommendation for tech gadgets as Christmas gifts. Consider the following criteria:\n",
    "\n",
    "    1. The content implies that it will mention technology products or gadgets.\n",
    "    2. It could be a review, list, or showcase of multiple tech items suitable for gifting.\n",
    "\n",
    "    Content:\n",
    "    {content}\n",
    "\n",
    "    Based on these criteria, is this video likely a tech product review, unboxing, or positive commentary on tech gadgets suitable for Christmas gifts?\n",
    "    \n",
    "    Answer with ONLY 'yes' or 'no'.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"solar-pro\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.lower().strip() == 'yes'\n",
    "\n",
    "# Assuming top_25_percent is already loaded as a pandas DataFrame\n",
    "tech_review_videos = []\n",
    "\n",
    "for _, row in top_25_percent.iterrows():\n",
    "    if is_tech_review(row['title']): \n",
    "        tech_review_videos.append(row['videoId'])\n",
    "\n",
    "print(f\"Found {len(tech_review_videos)} videos that are likely tech product reviews or unboxings:\")\n",
    "print(tech_review_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cdcf32ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=zijUjJdegI8\n",
      "[youtube] zijUjJdegI8: Downloading webpage\n",
      "[youtube] zijUjJdegI8: Downloading ios player API JSON\n",
      "[youtube] zijUjJdegI8: Downloading web creator player API JSON\n",
      "[youtube] zijUjJdegI8: Downloading m3u8 information\n",
      "[info] zijUjJdegI8: Downloading 1 format(s): 251\n",
      "[download] Destination: downloaded_videos/zijUjJdegI8.webm\n",
      "[download] 100% of    9.93MiB in 00:00:01 at 6.02MiB/s     \n",
      "[ExtractAudio] Destination: downloaded_videos/zijUjJdegI8.mp3\n",
      "Deleting original file downloaded_videos/zijUjJdegI8.webm (pass -k to keep)\n",
      "Successfully downloaded: Samsung Z Flip/Fold 6, Watch Ultra, Buds Pro and Ring Impressions! as MP3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yt_dlp\n",
    "from pydub import AudioSegment\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "# Constants\n",
    "CHUNK_SIZE = 25 * 1024 * 1024  # 25 MB, Whisper's file size limit\n",
    "\n",
    "def download_youtube_audio(video_id):\n",
    "    video_url = f'https://www.youtube.com/watch?v={video_id}'\n",
    "    output_path = 'downloaded_videos'\n",
    "    \n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'mp3',\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "        'outtmpl': f'{output_path}/{video_id}.%(ext)s',\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        try:\n",
    "            info = ydl.extract_info(video_url, download=True)\n",
    "            print(f\"Successfully downloaded: {info['title']} as MP3\")\n",
    "            return f'{output_path}/{video_id}.mp3', info['duration']\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download. Error: {str(e)}\")\n",
    "            return None, 0\n",
    "\n",
    "def split_audio(audio_path):\n",
    "    audio = AudioSegment.from_mp3(audio_path)\n",
    "    chunks = []\n",
    "    for i in range(0, len(audio), CHUNK_SIZE):\n",
    "        chunk = audio[i:i + CHUNK_SIZE]\n",
    "        chunk_path = f\"{audio_path[:-4]}_{i // CHUNK_SIZE}.mp3\"\n",
    "        chunk.export(chunk_path, format=\"mp3\")\n",
    "        chunks.append(chunk_path)\n",
    "    return chunks\n",
    "\n",
    "def transcribe_audio_chunk(chunk_path, client):\n",
    "    try:\n",
    "        with open(chunk_path, \"rb\") as audio_file:\n",
    "            transcript = client.audio.transcriptions.create(\n",
    "                model=\"whisper-1\", \n",
    "                file=audio_file\n",
    "            )\n",
    "        return transcript.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error in transcription: {str(e)}\")\n",
    "        return f\"Transcription failed: {str(e)}\"\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"OPENAI_API_KEY is not set in environment variables\")\n",
    "        return \"Transcription failed: No API key\"\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    file_size = os.path.getsize(audio_path)\n",
    "    if file_size > CHUNK_SIZE:\n",
    "        chunks = split_audio(audio_path)\n",
    "    else:\n",
    "        chunks = [audio_path]\n",
    "\n",
    "    transcriptions = []\n",
    "    for chunk in chunks:\n",
    "        transcription = transcribe_audio_chunk(chunk, client)\n",
    "        transcriptions.append(transcription)\n",
    "        if chunk != audio_path:\n",
    "            os.remove(chunk)  # Remove temporary chunk files\n",
    "\n",
    "    return \" \".join(transcriptions)\n",
    "\n",
    "def process_video(video_id):\n",
    "    audio_path, _ = download_youtube_audio(video_id)\n",
    "    if audio_path:\n",
    "        transcription = transcribe_audio(audio_path)\n",
    "        os.remove(audio_path)  # Remove the audio file after transcription\n",
    "        return transcription\n",
    "    return None\n",
    "\n",
    "# Main execution\n",
    "transcription_list = []\n",
    "for video_id in tech_review_videos:\n",
    "    transcription = process_video(video_id)\n",
    "    transcription_list.append(transcription)\n",
    "    time.sleep(3)  # Add a short delay between video processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b29bc604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing JSON: Unterminated string starting at: line 38 column 18 (char 2380)\n",
      "Raw response: {\n",
      "  \"products\": [\n",
      "    {\n",
      "      \"name\": \"Samsung Galaxy Z Flip 6\",\n",
      "      \"sentiment_score\": 3,\n",
      "      \"summary\": \"The Samsung Galaxy Z Flip 6 is an updated version of its predecessor, with minimal changes and a slightly higher price. It features a slightly better hinge, new AI features, and more RAM, but its overall design remains similar.\",\n",
      "      \"verdict\": \"NEUTRAL\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Samsung Galaxy Z Fold 6\",\n",
      "      \"sentiment_score\": 3,\n",
      "      \"summary\": \"The Samsung Galaxy Z Fold 6 is another updated version of its predecessor, with minimal changes and a higher price. It has a slightly better hinge, new AI features, and a larger battery, but its overall design and crease are similar to the previous generation.\",\n",
      "      \"verdict\": \"NEUTRAL\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Samsung Galaxy Watch 7\",\n",
      "      \"sentiment_score\": 4,\n",
      "      \"summary\": \"The Samsung Galaxy Watch 7 is a refreshed version of its predecessor, with a new titanium body and Sapphire crystal display for the Ultra model. It features a larger battery, new GPS, and a customizable quick button. However, it is priced similarly to its predecessor.\",\n",
      "      \"verdict\": \"FAVORABLE\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Samsung Galaxy Buds 3\",\n",
      "      \"sentiment_score\": 3,\n",
      "      \"summary\": \"The Samsung Galaxy Buds 3 are new earbuds with an aesthetic similar to the first-generation AirPods. They offer good sound quality and battery life, but they are not significantly different from their predecessors.\",\n",
      "      \"verdict\": \"NEUTRAL\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Samsung Galaxy Buds 3 Pro\",\n",
      "      \"sentiment_score\": 3,\n",
      "      \"summary\": \"The Samsung Galaxy Buds 3 Pro are an upgraded version of the Buds 3, featuring adaptive noise cancellation, ambient mode, and soft tips. They are priced higher than the Buds 3 and offer some new features, but they are not significantly different from other high-end earbuds in the market.\",\n",
      "      \"verdict\": \"NEUTRAL\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Samsung Galaxy Aura Ring\",\n",
      "      \"sentiment_score\": 4,\n",
      "\n",
      "    \"summary\": \"The Samsung Galaxy Aura Ring is a new fitness tracker that you can wear 24/7. It features a minimal design, water resistance, and a seven-day battery life. It does not require a subscription, unlike its competitors, and it can be charged using a convenient case. The price is higher than some competitors, but it offers some unique features.\",\n",
      "      \"verdict\": \"FAVORABLE]}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up OpenAI client for Solar API\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"SOLAR_API_KEY\"),\n",
    "    base_url=\"https://api.upstage.ai/v1/solar\"\n",
    ")\n",
    "\n",
    "def analyze_transcription(transcription):\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following transcription of a tech review video and provide a summary for each distinct product mentioned. Format your response as a valid JSON object with the following structure:\n",
    "\n",
    "    {{\n",
    "        \"products\": [\n",
    "            {{\n",
    "                \"name\": \"Product Name\",\n",
    "                \"sentiment_score\": 7,\n",
    "                \"summary\": \"Brief summary of the product review.\",\n",
    "                \"verdict\": \"FAVORABLE\"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "\n",
    "    Rules:\n",
    "    1. The \"products\" key should contain an array of product objects.\n",
    "    2. Each product object should have \"name\", \"sentiment_score\", \"summary\", and \"verdict\" keys.\n",
    "    3. \"sentiment_score\" should be an integer from 1 to 10.\n",
    "    4. \"verdict\" should be either \"FAVORABLE\", \"UNFAVORABLE\", or \"NEUTRAL\".\n",
    "    5. Ensure all string values are properly escaped for JSON.\n",
    "    6. The summary should be concise, focusing on key points that influenced the sentiment score.\n",
    "    7. Limit your response to a maximum of 5 products to ensure completeness.\n",
    "\n",
    "    Transcription:\n",
    "    {transcription}\n",
    "\n",
    "    Provide your analysis as a valid JSON object:\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"solar-pro\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    # Post-process the response to ensure valid JSON\n",
    "    raw_response = response.choices[0].message.content.strip()\n",
    "    try:\n",
    "        # Try to parse the JSON as is\n",
    "        return json.loads(raw_response)\n",
    "    except json.JSONDecodeError:\n",
    "        # If parsing fails, attempt to fix common issues\n",
    "        try:\n",
    "            # Add closing brackets if they're missing\n",
    "            if not raw_response.endswith('}}'):\n",
    "                raw_response += ']}}'\n",
    "            # Remove any trailing commas before closing brackets\n",
    "            raw_response = raw_response.replace(',]', ']').replace(',}', '}')\n",
    "            return json.loads(raw_response)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "            print(\"Raw response:\", raw_response)\n",
    "            return {\"products\": []}\n",
    "\n",
    "# Assuming transcription_list contains your transcriptions\n",
    "results = []\n",
    "\n",
    "for transcription in transcription_list:\n",
    "    analysis = analyze_transcription(transcription)\n",
    "    results.extend(analysis.get(\"products\", []))\n",
    "\n",
    "# Print or process the combined results\n",
    "for product in results:\n",
    "    print(f\"Product: {product['name']}\")\n",
    "    print(f\"Sentiment Score: {product['sentiment_score']}\")\n",
    "    print(f\"Summary: {product['summary']}\")\n",
    "    print(f\"Verdict: {product['verdict']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e5ee3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
